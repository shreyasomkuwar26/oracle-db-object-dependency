{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyphCYQ6UQ7u9Y1vOoDCWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyasomkuwar26/oracle-db-object-dependency/blob/main/Data_Engineering_Questionnarire.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Technical Interview Questionnaire\n",
        "**Target: 4-7 Years Experience | Data Engineering & Cloud Technologies**\n",
        "\n",
        "---\n",
        "\n",
        "## **SQL & PL/SQL Oracle**\n",
        "\n",
        "### **Easy**\n",
        "\n",
        "**Q1. What is the difference between DELETE, TRUNCATE, and DROP commands?**\n",
        "- **DELETE**: DML command, removes rows based on WHERE clause, can be rolled back, triggers fire, slower\n",
        "- **TRUNCATE**: DDL command, removes all rows, cannot be rolled back (in most cases), triggers don't fire, faster, resets high water mark\n",
        "- **DROP**: DDL command, removes the entire table structure and data, cannot be rolled back\n",
        "\n",
        "**Q2. Write a query to find the second highest salary from an Employee table.**\n",
        "```sql\n",
        "SELECT MAX(salary)\n",
        "FROM Employee\n",
        "WHERE salary < (SELECT MAX(salary) FROM Employee);\n",
        "\n",
        "-- Alternative using ROW_NUMBER\n",
        "SELECT salary FROM (\n",
        "  SELECT salary, ROW_NUMBER() OVER (ORDER BY salary DESC) as rn\n",
        "  FROM Employee\n",
        ") WHERE rn = 2;\n",
        "```\n",
        "\n",
        "**Q3. What are the different types of joins in SQL? Explain with examples.**\n",
        "- **INNER JOIN**: Returns matching rows from both tables\n",
        "- **LEFT JOIN**: Returns all rows from left table and matching rows from right\n",
        "- **RIGHT JOIN**: Returns all rows from right table and matching rows from left\n",
        "- **FULL OUTER JOIN**: Returns all rows from both tables\n",
        "- **CROSS JOIN**: Cartesian product of both tables\n",
        "\n",
        "### **Medium**\n",
        "\n",
        "**Q4. Explain the difference between RANK(), DENSE_RANK(), and ROW_NUMBER() window functions.**\n",
        "- **ROW_NUMBER()**: Assigns unique sequential integers (1,2,3,4...)\n",
        "- **RANK()**: Assigns rank with gaps for ties (1,2,2,4...)\n",
        "- **DENSE_RANK()**: Assigns rank without gaps for ties (1,2,2,3...)\n",
        "\n",
        "Example: For salaries [100, 90, 90, 80]\n",
        "- ROW_NUMBER: 1,2,3,4\n",
        "- RANK: 1,2,2,4\n",
        "- DENSE_RANK: 1,2,2,3\n",
        "\n",
        "**Q5. Write a PL/SQL block to handle exceptions when inserting duplicate records.**\n",
        "```sql\n",
        "DECLARE\n",
        "  v_emp_id NUMBER := 101;\n",
        "  duplicate_error EXCEPTION;\n",
        "  PRAGMA EXCEPTION_INIT(duplicate_error, -00001);\n",
        "BEGIN\n",
        "  INSERT INTO employees VALUES (v_emp_id, 'John Doe', 50000);\n",
        "  COMMIT;\n",
        "EXCEPTION\n",
        "  WHEN duplicate_error THEN\n",
        "    DBMS_OUTPUT.PUT_LINE('Employee ID already exists');\n",
        "  WHEN OTHERS THEN\n",
        "    DBMS_OUTPUT.PUT_LINE('Error: ' || SQLERRM);\n",
        "    ROLLBACK;\n",
        "END;\n",
        "```\n",
        "\n",
        "**Q6. Scenario: You need to find employees who haven't made any sales in the last 6 months. Write the query.**\n",
        "```sql\n",
        "SELECT e.emp_id, e.emp_name\n",
        "FROM employees e\n",
        "WHERE NOT EXISTS (\n",
        "  SELECT 1\n",
        "  FROM sales s\n",
        "  WHERE s.emp_id = e.emp_id\n",
        "  AND s.sale_date >= ADD_MONTHS(SYSDATE, -6)\n",
        ");\n",
        "```\n",
        "\n",
        "### **Hard**\n",
        "\n",
        "**Q7. Explain the concept of Materialized Views and when would you use them over regular views?**\n",
        "- **Materialized Views**: Physical copy of query results stored on disk, can be refreshed on demand or schedule\n",
        "- **Use Cases**:\n",
        "  - Complex aggregations on large datasets\n",
        "  - Queries with heavy joins\n",
        "  - Reports requiring fast response times\n",
        "  - Data warehouse scenarios with periodic refreshes\n",
        "- **Advantages**: Faster query performance, reduced computation\n",
        "- **Disadvantages**: Storage overhead, data may be stale between refreshes\n",
        "\n",
        "**Q8. Write a PL/SQL procedure with bulk collect and FORALL to efficiently update 1 million records.**\n",
        "```sql\n",
        "CREATE OR REPLACE PROCEDURE update_salary_bulk IS\n",
        "  TYPE emp_tab IS TABLE OF employees%ROWTYPE;\n",
        "  v_employees emp_tab;\n",
        "  \n",
        "  CURSOR c_emp IS\n",
        "    SELECT * FROM employees WHERE department_id = 10;\n",
        "BEGIN\n",
        "  OPEN c_emp;\n",
        "  LOOP\n",
        "    FETCH c_emp BULK COLLECT INTO v_employees LIMIT 10000;\n",
        "    EXIT WHEN v_employees.COUNT = 0;\n",
        "    \n",
        "    FORALL i IN 1..v_employees.COUNT\n",
        "      UPDATE employees\n",
        "      SET salary = v_employees(i).salary * 1.1\n",
        "      WHERE emp_id = v_employees(i).emp_id;\n",
        "    \n",
        "    COMMIT;\n",
        "  END LOOP;\n",
        "  CLOSE c_emp;\n",
        "END;\n",
        "```\n",
        "\n",
        "**Q9. Scenario: Design a solution to capture and log all DML operations on a critical table without impacting performance.**\n",
        "\n",
        "**Answer**: Implement Database Triggers with Autonomous Transactions\n",
        "```sql\n",
        "CREATE TABLE audit_log (\n",
        "  table_name VARCHAR2(50),\n",
        "  operation VARCHAR2(10),\n",
        "  old_value CLOB,\n",
        "  new_value CLOB,\n",
        "  username VARCHAR2(50),\n",
        "  change_date DATE\n",
        ");\n",
        "\n",
        "CREATE OR REPLACE TRIGGER trg_employee_audit\n",
        "AFTER INSERT OR UPDATE OR DELETE ON employees\n",
        "FOR EACH ROW\n",
        "DECLARE\n",
        "  PRAGMA AUTONOMOUS_TRANSACTION;\n",
        "BEGIN\n",
        "  IF INSERTING THEN\n",
        "    INSERT INTO audit_log VALUES ('EMPLOYEES', 'INSERT', NULL,\n",
        "      :NEW.emp_id||','||:NEW.salary, USER, SYSDATE);\n",
        "  ELSIF UPDATING THEN\n",
        "    INSERT INTO audit_log VALUES ('EMPLOYEES', 'UPDATE',\n",
        "      :OLD.emp_id||','||:OLD.salary, :NEW.emp_id||','||:NEW.salary,\n",
        "      USER, SYSDATE);\n",
        "  ELSIF DELETING THEN\n",
        "    INSERT INTO audit_log VALUES ('EMPLOYEES', 'DELETE',\n",
        "      :OLD.emp_id||','||:OLD.salary, NULL, USER, SYSDATE);\n",
        "  END IF;\n",
        "  COMMIT;\n",
        "END;\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Python**\n",
        "\n",
        "### **Easy**\n",
        "\n",
        "**Q10. What is the difference between list and tuple in Python?**\n",
        "- **List**: Mutable, defined with [], slower, methods like append/remove\n",
        "- **Tuple**: Immutable, defined with (), faster, used as dictionary keys\n",
        "- **Use Case**: Lists for collections that change, tuples for fixed data\n",
        "\n",
        "**Q11. Explain list comprehension with an example.**\n",
        "```python\n",
        "# Traditional approach\n",
        "squares = []\n",
        "for i in range(10):\n",
        "    squares.append(i**2)\n",
        "\n",
        "# List comprehension\n",
        "squares = [i**2 for i in range(10)]\n",
        "\n",
        "# With condition\n",
        "even_squares = [i**2 for i in range(10) if i % 2 == 0]\n",
        "```\n",
        "\n",
        "**Q12. What are *args and **kwargs? Provide examples.**\n",
        "```python\n",
        "# *args - variable positional arguments\n",
        "def sum_all(*args):\n",
        "    return sum(args)\n",
        "\n",
        "print(sum_all(1, 2, 3, 4))  # Output: 10\n",
        "\n",
        "# **kwargs - variable keyword arguments\n",
        "def print_info(**kwargs):\n",
        "    for key, value in kwargs.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "print_info(name=\"John\", age=30, city=\"NYC\")\n",
        "```\n",
        "\n",
        "### **Medium**\n",
        "\n",
        "**Q13. Explain the difference between deep copy and shallow copy.**\n",
        "```python\n",
        "import copy\n",
        "\n",
        "original = [[1, 2, 3], [4, 5, 6]]\n",
        "\n",
        "# Shallow copy - copies object reference\n",
        "shallow = copy.copy(original)\n",
        "shallow[0][0] = 99\n",
        "print(original)  # [[99, 2, 3], [4, 5, 6]] - original changed!\n",
        "\n",
        "# Deep copy - creates independent copy\n",
        "original = [[1, 2, 3], [4, 5, 6]]\n",
        "deep = copy.deepcopy(original)\n",
        "deep[0][0] = 99\n",
        "print(original)  # [[1, 2, 3], [4, 5, 6]] - original unchanged\n",
        "```\n",
        "\n",
        "**Q14. What are Python decorators? Write a decorator to measure function execution time.**\n",
        "```python\n",
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "def timing_decorator(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@timing_decorator\n",
        "def process_data():\n",
        "    time.sleep(2)\n",
        "    return \"Done\"\n",
        "```\n",
        "\n",
        "**Q15. Scenario: You have a list of dictionaries representing employee records. Filter employees with salary > 50000 and sort by name.**\n",
        "```python\n",
        "employees = [\n",
        "    {'name': 'John', 'salary': 60000, 'dept': 'IT'},\n",
        "    {'name': 'Alice', 'salary': 45000, 'dept': 'HR'},\n",
        "    {'name': 'Bob', 'salary': 75000, 'dept': 'IT'}\n",
        "]\n",
        "\n",
        "# Solution\n",
        "filtered = sorted(\n",
        "    [emp for emp in employees if emp['salary'] > 50000],\n",
        "    key=lambda x: x['name']\n",
        ")\n",
        "\n",
        "# Using filter and sorted\n",
        "filtered = sorted(\n",
        "    filter(lambda x: x['salary'] > 50000, employees),\n",
        "    key=lambda x: x['name']\n",
        ")\n",
        "```\n",
        "\n",
        "### **Hard**\n",
        "\n",
        "**Q16. Explain Python's GIL (Global Interpreter Lock) and its implications.**\n",
        "- **GIL**: Mutex that protects access to Python objects, prevents multiple threads from executing Python bytecode simultaneously\n",
        "- **Implications**:\n",
        "  - CPU-bound multi-threaded programs don't benefit from multiple cores\n",
        "  - I/O-bound operations can still benefit from threading\n",
        "  - Use multiprocessing for CPU-intensive tasks\n",
        "  - Use asyncio for I/O-intensive tasks\n",
        "\n",
        "**Q17. Implement a context manager for database connection handling.**\n",
        "```python\n",
        "class DatabaseConnection:\n",
        "    def __init__(self, host, port):\n",
        "        self.host = host\n",
        "        self.port = port\n",
        "        self.connection = None\n",
        "    \n",
        "    def __enter__(self):\n",
        "        # Establish connection\n",
        "        self.connection = self._connect()\n",
        "        print(f\"Connected to {self.host}:{self.port}\")\n",
        "        return self.connection\n",
        "    \n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        # Close connection\n",
        "        if self.connection:\n",
        "            self.connection.close()\n",
        "            print(\"Connection closed\")\n",
        "        if exc_type:\n",
        "            print(f\"Exception occurred: {exc_val}\")\n",
        "        return False  # Propagate exceptions\n",
        "    \n",
        "    def _connect(self):\n",
        "        # Simulated connection\n",
        "        return {\"status\": \"connected\"}\n",
        "\n",
        "# Usage\n",
        "with DatabaseConnection(\"localhost\", 5432) as conn:\n",
        "    # Work with connection\n",
        "    pass\n",
        "```\n",
        "\n",
        "**Q18. Scenario: Design a solution to process a 10GB CSV file that doesn't fit in memory.**\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "def process_large_file(file_path, chunk_size=100000):\n",
        "    \"\"\"\n",
        "    Process large CSV in chunks\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Read in chunks\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "        # Process each chunk\n",
        "        processed = chunk[chunk['amount'] > 1000]\n",
        "        aggregated = processed.groupby('category')['amount'].sum()\n",
        "        results.append(aggregated)\n",
        "    \n",
        "    # Combine results\n",
        "    final_result = pd.concat(results).groupby(level=0).sum()\n",
        "    return final_result\n",
        "\n",
        "# Alternative: Using generators\n",
        "def process_with_generator(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        header = next(f)\n",
        "        for line in f:\n",
        "            # Process line by line\n",
        "            yield process_line(line)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **PySpark**\n",
        "\n",
        "### **Easy**\n",
        "\n",
        "**Q19. What is the difference between transformation and action in Spark?**\n",
        "- **Transformations**: Lazy operations that create new RDD/DataFrame (map, filter, select, groupBy)\n",
        "  - Not executed immediately\n",
        "  - Create DAG (Directed Acyclic Graph)\n",
        "- **Actions**: Trigger execution and return results (collect, count, show, save)\n",
        "  - Execute all transformations in lineage\n",
        "- **Example**:\n",
        "  - `df.filter()` - transformation\n",
        "  - `df.count()` - action\n",
        "\n",
        "**Q20. Write PySpark code to read a CSV file and display first 10 rows.**\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ReadCSV\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"s3://bucket/data.csv\")\n",
        "\n",
        "df.show(10)\n",
        "```\n",
        "\n",
        "**Q21. What is the difference between repartition() and coalesce()?**\n",
        "- **repartition()**: Full shuffle, can increase or decrease partitions, distributes data evenly\n",
        "- **coalesce()**: Minimizes shuffle, only decreases partitions, moves data to fewer partitions\n",
        "- **Use Case**: Use coalesce for reducing partitions efficiently, repartition when need even distribution\n",
        "\n",
        "### **Medium**\n",
        "\n",
        "**Q22. Explain the difference between map() and flatMap() transformations.**\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "rdd = spark.sparkContext.parallelize([\"Hello World\", \"PySpark Tutorial\"])\n",
        "\n",
        "# map - one-to-one mapping\n",
        "mapped = rdd.map(lambda x: x.split())\n",
        "print(mapped.collect())  \n",
        "# [['Hello', 'World'], ['PySpark', 'Tutorial']]\n",
        "\n",
        "# flatMap - one-to-many, flattens result\n",
        "flat_mapped = rdd.flatMap(lambda x: x.split())\n",
        "print(flat_mapped.collect())  \n",
        "# ['Hello', 'World', 'PySpark', 'Tutorial']\n",
        "```\n",
        "\n",
        "**Q23. Write PySpark code to perform an inner join between two DataFrames.**\n",
        "```python\n",
        "# Sample DataFrames\n",
        "employees = spark.createDataFrame([\n",
        "    (1, \"John\", 10),\n",
        "    (2, \"Alice\", 20),\n",
        "    (3, \"Bob\", 10)\n",
        "], [\"emp_id\", \"name\", \"dept_id\"])\n",
        "\n",
        "departments = spark.createDataFrame([\n",
        "    (10, \"IT\"),\n",
        "    (20, \"HR\"),\n",
        "    (30, \"Finance\")\n",
        "], [\"dept_id\", \"dept_name\"])\n",
        "\n",
        "# Inner Join\n",
        "result = employees.join(\n",
        "    departments,\n",
        "    employees.dept_id == departments.dept_id,\n",
        "    \"inner\"\n",
        ").select(\n",
        "    employees.emp_id,\n",
        "    employees.name,\n",
        "    departments.dept_name\n",
        ")\n",
        "\n",
        "result.show()\n",
        "```\n",
        "\n",
        "**Q24. Scenario: You need to find duplicate records in a dataset based on multiple columns. Write the code.**\n",
        "```python\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import col, count\n",
        "\n",
        "# Method 1: Using groupBy\n",
        "duplicates = df.groupBy(\"col1\", \"col2\", \"col3\") \\\n",
        "    .count() \\\n",
        "    .filter(col(\"count\") > 1)\n",
        "\n",
        "# Method 2: Using window function\n",
        "window_spec = Window.partitionBy(\"col1\", \"col2\", \"col3\")\n",
        "\n",
        "df_with_count = df.withColumn(\n",
        "    \"duplicate_count\",\n",
        "    count(\"*\").over(window_spec)\n",
        ")\n",
        "\n",
        "duplicates = df_with_count.filter(col(\"duplicate_count\") > 1)\n",
        "```\n",
        "\n",
        "### **Hard**\n",
        "\n",
        "**Q25. Explain Spark's execution model: Driver, Executors, and how tasks are distributed.**\n",
        "- **Driver**: Main program, creates SparkContext, converts user program to tasks, schedules tasks\n",
        "- **Cluster Manager**: Allocates resources (YARN, Mesos, Kubernetes, Standalone)\n",
        "- **Executors**: Worker nodes, execute tasks, store data for caching\n",
        "- **Execution Flow**:\n",
        "  1. Driver creates logical plan (DAG)\n",
        "  2. DAG Scheduler divides into stages\n",
        "  3. Task Scheduler assigns tasks to executors\n",
        "  4. Executors run tasks and return results\n",
        "\n",
        "**Q26. Write optimized PySpark code to handle slowly changing dimensions (SCD Type 2).**\n",
        "```python\n",
        "from pyspark.sql.functions import col, lit, current_timestamp, when\n",
        "\n",
        "def process_scd_type2(source_df, target_df, key_cols, compare_cols):\n",
        "    \"\"\"\n",
        "    Implements SCD Type 2 logic\n",
        "    \"\"\"\n",
        "    # Join source and target\n",
        "    joined = source_df.alias(\"src\").join(\n",
        "        target_df.alias(\"tgt\"),\n",
        "        key_cols,\n",
        "        \"left\"\n",
        "    )\n",
        "    \n",
        "    # Identify changes\n",
        "    change_condition = None\n",
        "    for col_name in compare_cols:\n",
        "        condition = col(f\"src.{col_name}\") != col(f\"tgt.{col_name}\")\n",
        "        change_condition = condition if change_condition is None else change_condition | condition\n",
        "    \n",
        "    # New records\n",
        "    new_records = joined.filter(col(\"tgt.id\").isNull()) \\\n",
        "        .select(\"src.*\") \\\n",
        "        .withColumn(\"effective_date\", current_timestamp()) \\\n",
        "        .withColumn(\"end_date\", lit(None)) \\\n",
        "        .withColumn(\"is_current\", lit(True))\n",
        "    \n",
        "    # Changed records - expire old\n",
        "    expired_records = joined.filter(change_condition) \\\n",
        "        .select(\"tgt.*\") \\\n",
        "        .withColumn(\"end_date\", current_timestamp()) \\\n",
        "        .withColumn(\"is_current\", lit(False))\n",
        "    \n",
        "    # Changed records - insert new\n",
        "    updated_records = joined.filter(change_condition) \\\n",
        "        .select(\"src.*\") \\\n",
        "        .withColumn(\"effective_date\", current_timestamp()) \\\n",
        "        .withColumn(\"end_date\", lit(None)) \\\n",
        "        .withColumn(\"is_current\", lit(True))\n",
        "    \n",
        "    # Union all\n",
        "    result = new_records.union(updated_records).union(expired_records)\n",
        "    \n",
        "    return result\n",
        "```\n",
        "\n",
        "**Q27. Scenario: Optimize a PySpark job that's running slow due to data skew in a join operation.**\n",
        "```python\n",
        "from pyspark.sql.functions import rand, concat, lit\n",
        "\n",
        "def handle_data_skew(df1, df2, join_key):\n",
        "    \"\"\"\n",
        "    Handle skewed join using salting technique\n",
        "    \"\"\"\n",
        "    # Add salt to larger DataFrame\n",
        "    salt_range = 10\n",
        "    df1_salted = df1.withColumn(\n",
        "        \"salt\",\n",
        "        (rand() * salt_range).cast(\"int\")\n",
        "    ).withColumn(\n",
        "        \"salted_key\",\n",
        "        concat(col(join_key), lit(\"_\"), col(\"salt\"))\n",
        "    )\n",
        "    \n",
        "    # Replicate smaller DataFrame\n",
        "    df2_replicated = df2.withColumn(\n",
        "        \"salt\",\n",
        "        explode(array([lit(i) for i in range(salt_range)]))\n",
        "    ).withColumn(\n",
        "        \"salted_key\",\n",
        "        concat(col(join_key), lit(\"_\"), col(\"salt\"))\n",
        "    )\n",
        "    \n",
        "    # Perform join on salted key\n",
        "    result = df1_salted.join(\n",
        "        df2_replicated,\n",
        "        \"salted_key\"\n",
        "    ).drop(\"salt\", \"salted_key\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Additional optimizations:\n",
        "# 1. Broadcast smaller table if < 10MB\n",
        "# result = df1.join(broadcast(df2), join_key)\n",
        "\n",
        "# 2. Repartition before join\n",
        "# df1 = df1.repartition(200, join_key)\n",
        "\n",
        "# 3. Cache frequently used DataFrames\n",
        "# df1.cache()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **AWS Cloud**\n",
        "\n",
        "### **Easy**\n",
        "\n",
        "**Q28. What is the difference between S3 Standard and S3 Glacier storage classes?**\n",
        "- **S3 Standard**: Frequent access, millisecond latency, higher cost, 99.99% availability\n",
        "- **S3 Glacier**: Archive storage, minutes-hours retrieval time, lower cost, long-term retention\n",
        "- **Use Case**: Standard for active data, Glacier for compliance/archival\n",
        "\n",
        "**Q29. Explain the difference between IAM Roles and IAM Users.**\n",
        "- **IAM Users**: Permanent credentials, for specific people/applications, long-term access\n",
        "- **IAM Roles**: Temporary credentials, can be assumed by users/services, short-term access\n",
        "- **Best Practice**: Use roles for applications, services, and cross-account access\n",
        "\n",
        "**Q30. What are the different EC2 instance types and their use cases?**\n",
        "- **General Purpose (t3, m5)**: Balanced compute/memory/network\n",
        "- **Compute Optimized (c5)**: High-performance processors\n",
        "- **Memory Optimized (r5)**: Large datasets in memory\n",
        "- **Storage Optimized (i3)**: High sequential read/write\n",
        "- **GPU Instances (p3)**: Machine learning, graphics rendering\n",
        "\n",
        "### **Medium**\n",
        "\n",
        "**Q31. Design a solution for secure data transfer from on-premises to S3.**\n",
        "\n",
        "**Answer**:\n",
        "- **Option 1: AWS Direct Connect** - Dedicated network connection, consistent performance\n",
        "- **Option 2: AWS DataSync** - Automated data transfer, encryption in transit\n",
        "- **Option 3: AWS Snowball** - Physical device for large-scale migration\n",
        "- **Security**:\n",
        "  - Use SSL/TLS for encryption in transit\n",
        "  - Enable S3 server-side encryption (SSE-S3, SSE-KMS)\n",
        "  - Implement bucket policies and IAM roles\n",
        "  - Enable S3 versioning and MFA delete\n",
        "\n",
        "**Q32. Explain VPC components: Subnets, Route Tables, Internet Gateway, NAT Gateway.**\n",
        "- **VPC**: Isolated network in AWS cloud\n",
        "- **Subnets**: IP address ranges within VPC (public/private)\n",
        "- **Internet Gateway**: Allows internet access for public subnets\n",
        "- **NAT Gateway**: Allows private subnet instances to access internet\n",
        "- **Route Tables**: Define traffic routing rules\n",
        "- **Security Groups**: Instance-level firewall\n",
        "- **NACLs**: Subnet-level firewall\n",
        "\n",
        "**Q33. Scenario: Your application on EC2 needs to read from S3. What's the secure way to grant access?**\n",
        "\n",
        "**Answer**: Use IAM Roles\n",
        "```python\n",
        "# Steps:\n",
        "# 1. Create IAM role with S3 read policy\n",
        "{\n",
        "    \"Version\": \"2012-10-17\",\n",
        "    \"Statement\": [{\n",
        "        \"Effect\": \"Allow\",\n",
        "        \"Action\": [\n",
        "            \"s3:GetObject\",\n",
        "            \"s3:ListBucket\"\n",
        "        ],\n",
        "        \"Resource\": [\n",
        "            \"arn:aws:s3:::my-bucket/*\",\n",
        "            \"arn:aws:s3:::my-bucket\"\n",
        "        ]\n",
        "    }]\n",
        "}\n",
        "\n",
        "# 2. Attach role to EC2 instance\n",
        "# 3. Application uses AWS SDK with default credentials\n",
        "import boto3\n",
        "\n",
        "s3 = boto3.client('s3')  # Automatically uses instance role\n",
        "response = s3.get_object(Bucket='my-bucket', Key='data.csv')\n",
        "```\n",
        "\n",
        "### **Hard**\n",
        "\n",
        "**Q34. Design a highly available and fault-tolerant architecture for a data processing application.**\n",
        "\n",
        "**Answer**:\n",
        "```\n",
        "Architecture Components:\n",
        "1. Multi-AZ deployment across 3 availability zones\n",
        "2. Application Load Balancer (ALB) for traffic distribution\n",
        "3. Auto Scaling Group for EC2 instances (min: 2, max: 10)\n",
        "4. RDS Multi-AZ for database high availability\n",
        "5. S3 for data storage with versioning enabled\n",
        "6. ElastiCache for caching layer\n",
        "7. CloudWatch for monitoring and alarms\n",
        "8. Route 53 for DNS with health checks\n",
        "\n",
        "Data Flow:\n",
        "Users → Route 53 → ALB → EC2 (Multi-AZ) → RDS Multi-AZ\n",
        "                              ↓\n",
        "                        ElastiCache → S3\n",
        "\n",
        "Disaster Recovery:\n",
        "- Automated backups to S3\n",
        "- Cross-region replication for critical data\n",
        "- RTO: 1 hour, RPO: 15 minutes\n",
        "```\n",
        "\n",
        "**Q35. Explain AWS Lambda cold start and strategies to minimize it.**\n",
        "\n",
        "**Answer**:\n",
        "- **Cold Start**: Initialization time when new container is created (package loading, runtime initialization)\n",
        "- **Impact**: 100ms - 3+ seconds delay\n",
        "\n",
        "**Mitigation Strategies**:\n",
        "1. **Provisioned Concurrency**: Pre-initialized execution environments\n",
        "2. **Keep functions warm**: Scheduled CloudWatch Events every 5 minutes\n",
        "3. **Reduce package size**: Remove unnecessary dependencies, use layers\n",
        "4. **Choose optimal runtime**: Lower-level languages (Go, Java) after warmup are faster\n",
        "5. **Optimize code**: Minimize initialization code, lazy load dependencies\n",
        "6. **Increase memory**: More memory = more CPU = faster startup\n",
        "\n",
        "```python\n",
        "# Example: Lambda with optimization\n",
        "import json\n",
        "\n",
        "# Global scope - initialized once per container\n",
        "import boto3\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    # Handler code - runs on every invocation\n",
        "    bucket = event['bucket']\n",
        "    key = event['key']\n",
        "    \n",
        "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
        "    return {\n",
        "        'statusCode': 200,\n",
        "        'body': json.dumps('Success')\n",
        "    }\n",
        "```\n",
        "\n",
        "**Q36. Scenario: Design a cost-optimized data lake architecture on AWS.**\n",
        "\n",
        "**Answer**:\n",
        "```\n",
        "Architecture:\n",
        "1. Data Ingestion:\n",
        "   - AWS Kinesis Firehose → S3 (streaming data)\n",
        "   - AWS DataSync (batch from on-premises)\n",
        "   - S3 Transfer Acceleration for fast uploads\n",
        "\n",
        "2. Storage Layers:\n",
        "   - S3 Standard: Hot data (last 30 days)\n",
        "   - S3 Standard-IA: Warm data (30-90 days)\n",
        "   - S3 Glacier: Cold data (>90 days)\n",
        "   - S3 Intelligent-Tiering: Unknown access patterns\n",
        "\n",
        "3. Processing:\n",
        "   - AWS Glue: ETL jobs, data catalog\n",
        "   - Amazon Athena: Interactive queries\n",
        "   - EMR with Spot Instances: Large-scale processing\n",
        "\n",
        "4. Optimization:\n",
        "   - Lifecycle policies for automatic tiering\n",
        "   - Parquet/ORC format for better compression\n",
        "   - Partition data by date/region\n",
        "   - Use Glue Data Catalog for metadata\n",
        "   - S3 Select/Glacier Select for filtered retrieval\n",
        "\n",
        "5. Cost Monitoring:\n",
        "   - AWS Cost Explorer\n",
        "   - S3 Storage Lens\n",
        "   - CloudWatch metrics\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Databricks**\n",
        "\n",
        "### **Easy**\n",
        "\n",
        "**Q37. What is the difference between Databricks clusters and SQL Warehouses?**\n",
        "- **Clusters**: General-purpose compute for notebooks, jobs, streaming (Spark clusters)\n",
        "- **SQL Warehouses**: Optimized for SQL queries and BI tools, auto-scaling, serverless option\n",
        "- **Use Case**: Clusters for data engineering/ML, SQL Warehouses for analytics/BI\n",
        "\n",
        "**Q38. Explain Delta Lake and its key features.**\n",
        "- **Delta Lake**: Open-source storage layer on top of data lakes\n",
        "- **Features**:\n",
        "  - ACID transactions\n",
        "  - Time travel (data versioning)\n",
        "  - Schema enforcement and evolution\n",
        "  - Unified batch and streaming\n",
        "  - Audit history\n",
        "- **Benefits**: Reliability, performance, governance\n",
        "\n",
        "**Q39. Write code to create a Delta table in Databricks.**\n",
        "```python\n",
        "# Create Delta table from DataFrame\n",
        "df = spark.read.csv(\"/mnt/data/sales.csv\", header=True)\n",
        "\n",
        "df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .save(\"/mnt/delta/sales\")\n",
        "\n",
        "# Create managed Delta table\n",
        "df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .saveAsTable(\"sales\")\n",
        "\n",
        "# SQL syntax\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE sales\n",
        "    USING DELTA\n",
        "    LOCATION '/mnt/delta/sales'\n",
        "\"\"\")\n",
        "```\n",
        "\n",
        "### **Medium**\n",
        "\n",
        "**Q40. Explain the difference between Managed and External tables in Databricks.**\n",
        "- **Managed Tables**:\n",
        "  - Databricks manages both metadata and data\n",
        "  - Data stored in DBFS default location\n",
        "  - DROP TABLE deletes both metadata and data\n",
        "  \n",
        "- **External Tables**:\n",
        "  - Databricks manages only metadata\n",
        "  - Data stored in user-specified location\n",
        "  - DROP TABLE deletes only metadata, data remains\n",
        "\n",
        "**Q41. Write code to perform MERGE operation (UPSERT) in Delta Lake.**\n",
        "```python\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "# Target Delta table\n",
        "target = DeltaTable.forPath(spark, \"/mnt/delta/customers\")\n",
        "\n",
        "# Source updates\n",
        "updates = spark.read.csv(\"/mnt/updates/customers.csv\", header=True)\n",
        "\n",
        "# MERGE operation\n",
        "target.alias(\"target\").merge(\n",
        "    updates.alias(\"source\"),\n",
        "    \"target.customer_id = source.customer_id\"\n",
        ").whenMatchedUpdate(\n",
        "    set = {\n",
        "        \"name\": \"source.name\",\n",
        "        \"email\": \"source.email\",\n",
        "        \"updated_date\": \"current_timestamp()\"\n",
        "    }\n",
        ").whenNotMatchedInsert(\n",
        "    values = {\n",
        "        \"customer_id\": \"source.customer_id\",\n",
        "        \"name\": \"source.name\",\n",
        "        \"email\": \"source.email\",\n",
        "        \"created_date\": \"current_timestamp()\"\n",
        "    }\n",
        ").execute()\n",
        "```\n",
        "\n",
        "**Q42. Scenario: You need to recover data that was accidentally deleted 2 hours ago. How would you do it?**\n",
        "```python\n",
        "# Time Travel in Delta Lake\n",
        "\n",
        "# Method 1: Using version number\n",
        "df = spark.read.format(\"delta\") \\\n",
        "    .option(\"versionAsOf\", 10) \\\n",
        "    .load(\"/mnt/delta/sales\")\n",
        "\n",
        "# Method 2: Using timestamp\n",
        "df = spark.read.format(\"delta\") \\\n",
        "    .option(\"timestampAsOf\", \"2024-12-10 14:00:00\") \\\n",
        "    .load(\"/mnt/delta/sales\")\n",
        "\n",
        "# Restore deleted data\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "delta_table = DeltaTable.forPath(spark, \"/mnt/delta/sales\")\n",
        "\n",
        "# Restore to previous version\n",
        "delta_table.restoreToVersion(10)\n",
        "\n",
        "# Or restore to timestamp\n",
        "delta_table.restoreToTimestamp(\"2024-12-10 14:00:00\")\n",
        "\n",
        "# View history\n",
        "spark.sql(\"DESCRIBE HISTORY delta.`/mnt/delta/sales`\").show()\n",
        "```\n",
        "\n",
        "### **Hard**\n",
        "\n",
        "**Q43. Explain Databricks' Adaptive Query Execution (AQE) and its benefits.**\n",
        "\n",
        "**Answer**:\n",
        "- **AQE**: Runtime optimization of query plans based on actual data statistics\n",
        "- **Key Features**:\n",
        "  1. **Dynamically coalescing shuffle partitions**: Reduces partitions post-shuffle\n",
        "  2. **Dynamically switching join strategies**: Changes sort-merge to broadcast join\n",
        "  3. **Dynamically optimizing skew joins**: Splits skewed partitions\n",
        "\n",
        "```python\n",
        "# Enable AQE (enabled by default in Databricks Runtime 7.3+)\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
        "\n",
        "# Benefits:\n",
        "# - Improved query performance\n",
        "# - Better resource utilization\n",
        "# - Automatic optimization without manual tuning\n",
        "```\n",
        "\n",
        "**Q44. Design an incremental data processing pipeline using Delta Lake and Structured Streaming.**\n",
        "```python\n",
        "from pyspark.sql.functions import col, current_timestamp\n",
        "\n",
        "# Read streaming data from source\n",
        "streaming_df = spark.readStream \\\n",
        "    .format(\"cloudFiles\") \\\n",
        "    .option(\"cloudFiles.format\", \"json\") \\\n",
        "    .option(\"cloudFiles.schemaLocation\", \"/mnt/schema/\") \\\n",
        "    .load(\"/mnt/incoming/\")\n",
        "\n",
        "# Transform data\n",
        "transformed_df = streaming_df \\\n",
        "    .withColumn(\"processed_time\", current_timestamp()) \\\n",
        "    .filter(col(\"amount\") > 0)\n",
        "\n",
        "# Write to Delta Lake with checkpointing\n",
        "query = transformed_df.writeStream \\\n",
        "    .format(\"delta\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"checkpointLocation\", \"/mnt/checkpoints/sales\") \\\n",
        "    .trigger(processingTime=\"5 minutes\") \\\n",
        "    .start(\"/mnt/delta/sales\")\n",
        "\n",
        "# Downstream processing with Delta streaming\n",
        "delta_stream = spark.readStream \\\n",
        "    .format(\"delta\") \\\n",
        "    .load(\"/mnt/delta/sales\")\n",
        "\n",
        "# Aggregate and write to aggregated table\n",
        "aggregated = delta_stream \\\n",
        "    .groupBy(\"category\", window(col(\"processed_time\"), \"1 hour\")) \\\n",
        "    .agg({\"amount\": \"sum\", \"quantity\": \"sum\"})\n",
        "\n",
        "aggregated.writeStream \\\n",
        "    .format(\"delta\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .option(\"checkpointLocation\", \"/mnt/checkpoints/agg\") \\\n",
        "    .start(\"/mnt/delta/sales_hourly\")\n",
        "```\n",
        "\n",
        "**Q45. Scenario: Optimize a slow-performing Delta table with 500 million records. What strategies would you use?**\n",
        "\n",
        "**Answer**:\n",
        "```python\n",
        "# 1. OPTIMIZE - Compaction\n",
        "spark.sql(\"OPTIMIZE delta.`/mnt/delta/large_table`\")\n",
        "\n",
        "# 2. Z-Ordering for frequently filtered columns\n",
        "spark.sql(\"\"\"\n",
        "    OPTIMIZE delta.`/mnt/delta/large_table`"
      ],
      "metadata": {
        "id": "pIMGjot8xMNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "    ZORDER BY (date, customer_id, region)\n",
        "\"\"\")\n",
        "\n",
        "# 3. Partition by high-cardinality column\n",
        "df.write.format(\"delta\") \\\n",
        "    .partitionBy(\"date\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/mnt/delta/large_table_partitioned\")\n",
        "\n",
        "# 4. VACUUM to remove old files (after optimization)\n",
        "spark.sql(\"\"\"\n",
        "    VACUUM delta.`/mnt/delta/large_table`\n",
        "    RETAIN 168 HOURS\n",
        "\"\"\")\n",
        "\n",
        "# 5. Data skipping with statistics\n",
        "spark.sql(\"\"\"\n",
        "    ANALYZE TABLE delta.`/mnt/delta/large_table`\n",
        "    COMPUTE STATISTICS FOR ALL COLUMNS\n",
        "\"\"\")\n",
        "\n",
        "# 6. Enable Auto Optimize\n",
        "spark.sql(\"\"\"\n",
        "    ALTER TABLE delta.`/mnt/delta/large_table`\n",
        "    SET TBLPROPERTIES (\n",
        "        delta.autoOptimize.optimizeWrite = true,\n",
        "        delta.autoOptimize.autoCompact = true\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "# 7. Cache frequently accessed data\n",
        "spark.sql(\"CACHE TABLE large_table\")\n",
        "\n",
        "# 8. Use liquid clustering (Databricks Runtime 13.3+)\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE large_table_clustered\n",
        "    USING DELTA\n",
        "    CLUSTER BY (date, customer_id)\n",
        "    AS SELECT * FROM large_table\n",
        "\"\"\")\n",
        "\n",
        "# Performance Monitoring\n",
        "display(spark.sql(\"DESCRIBE DETAIL delta.`/mnt/delta/large_table`\"))\n",
        "display(spark.sql(\"DESCRIBE HISTORY delta.`/mnt/delta/large_table`\"))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Cross-Technology Integration Questions**\n",
        "\n",
        "### **Medium**\n",
        "\n",
        "**Q46. Scenario: Design an end-to-end pipeline to ingest data from Oracle database to Databricks, transform it, and load to S3 as Parquet.**\n",
        "\n",
        "**Answer**:\n",
        "```python\n",
        "# Step 1: Extract from Oracle using PySpark JDBC\n",
        "jdbc_url = \"jdbc:oracle:thin:@hostname:1521:ORCL\"\n",
        "connection_properties = {\n",
        "    \"user\": \"username\",\n",
        "    \"password\": \"password\",\n",
        "    \"driver\": \"oracle.jdbc.driver.OracleDriver\"\n",
        "}\n",
        "\n",
        "# Incremental load using watermark\n",
        "query = \"\"\"(\n",
        "    SELECT * FROM employees\n",
        "    WHERE last_modified_date > TO_DATE('2024-12-01', 'YYYY-MM-DD')\n",
        ") emp\"\"\"\n",
        "\n",
        "df_oracle = spark.read.jdbc(\n",
        "    url=jdbc_url,\n",
        "    table=query,\n",
        "    properties=connection_properties,\n",
        "    numPartitions=10,\n",
        "    column=\"emp_id\",\n",
        "    lowerBound=1,\n",
        "    upperBound=100000\n",
        ")\n",
        "\n",
        "# Step 2: Transform in Databricks\n",
        "from pyspark.sql.functions import col, when, upper, trim\n",
        "\n",
        "df_transformed = df_oracle \\\n",
        "    .withColumn(\"email\", upper(trim(col(\"email\")))) \\\n",
        "    .withColumn(\"salary_band\",\n",
        "        when(col(\"salary\") < 50000, \"Low\")\n",
        "        .when(col(\"salary\") < 100000, \"Medium\")\n",
        "        .otherwise(\"High\")\n",
        "    ) \\\n",
        "    .filter(col(\"status\") == \"ACTIVE\")\n",
        "\n",
        "# Step 3: Write to Delta Lake (staging)\n",
        "df_transformed.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/mnt/delta/employees_staging\")\n",
        "\n",
        "# Step 4: Apply business logic and write to S3 as Parquet\n",
        "df_final = spark.read.format(\"delta\") \\\n",
        "    .load(\"/mnt/delta/employees_staging\")\n",
        "\n",
        "df_final.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"department\", \"hire_year\") \\\n",
        "    .parquet(\"s3://my-bucket/employees/\")\n",
        "\n",
        "# Step 5: Create external table in Athena for querying\n",
        "spark.sql(\"\"\"\n",
        "    CREATE EXTERNAL TABLE IF NOT EXISTS employees_athena (\n",
        "        emp_id INT,\n",
        "        name STRING,\n",
        "        email STRING,\n",
        "        salary DECIMAL(10,2),\n",
        "        salary_band STRING\n",
        "    )\n",
        "    PARTITIONED BY (department STRING, hire_year INT)\n",
        "    STORED AS PARQUET\n",
        "    LOCATION 's3://my-bucket/employees/'\n",
        "\"\"\")\n",
        "\n",
        "# Orchestration with Databricks Workflows\n",
        "# Create job to run daily at 2 AM\n",
        "```\n",
        "\n",
        "**Q47. Write a Python script to automate the creation of AWS Glue crawlers for multiple S3 buckets.**\n",
        "\n",
        "**Answer**:\n",
        "```python\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "def create_glue_crawler(crawler_name, database_name, s3_path, iam_role):\n",
        "    \"\"\"\n",
        "    Create AWS Glue crawler for S3 data source\n",
        "    \"\"\"\n",
        "    glue_client = boto3.client('glue', region_name='us-east-1')\n",
        "    \n",
        "    try:\n",
        "        response = glue_client.create_crawler(\n",
        "            Name=crawler_name,\n",
        "            Role=iam_role,\n",
        "            DatabaseName=database_name,\n",
        "            Targets={\n",
        "                'S3Targets': [\n",
        "                    {\n",
        "                        'Path': s3_path,\n",
        "                        'Exclusions': ['**.tmp', '**/_temporary/**']\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "            Schedule='cron(0 2 * * ? *)',  # Daily at 2 AM\n",
        "            SchemaChangePolicy={\n",
        "                'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
        "                'DeleteBehavior': 'LOG'\n",
        "            },\n",
        "            RecrawlPolicy={\n",
        "                'RecrawlBehavior': 'CRAWL_NEW_FOLDERS_ONLY'\n",
        "            },\n",
        "            Configuration=json.dumps({\n",
        "                \"Version\": 1.0,\n",
        "                \"CrawlerOutput\": {\n",
        "                    \"Partitions\": {\"AddOrUpdateBehavior\": \"InheritFromTable\"}\n",
        "                }\n",
        "            })\n",
        "        )\n",
        "        print(f\"Crawler {crawler_name} created successfully\")\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating crawler: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Create crawlers for multiple buckets\n",
        "s3_buckets = [\n",
        "    {'name': 'sales-crawler', 'path': 's3://data-lake/sales/'},\n",
        "    {'name': 'customers-crawler', 'path': 's3://data-lake/customers/'},\n",
        "    {'name': 'products-crawler', 'path': 's3://data-lake/products/'}\n",
        "]\n",
        "\n",
        "iam_role = 'arn:aws:iam::123456789012:role/GlueServiceRole'\n",
        "database = 'data_lake_db'\n",
        "\n",
        "for bucket in s3_buckets:\n",
        "    create_glue_crawler(\n",
        "        crawler_name=bucket['name'],\n",
        "        database_name=database,\n",
        "        s3_path=bucket['path'],\n",
        "        iam_role=iam_role\n",
        "    )\n",
        "\n",
        "# Start all crawlers\n",
        "def start_all_crawlers(crawler_names):\n",
        "    glue_client = boto3.client('glue')\n",
        "    for crawler_name in crawler_names:\n",
        "        try:\n",
        "            glue_client.start_crawler(Name=crawler_name)\n",
        "            print(f\"Started crawler: {crawler_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error starting crawler {crawler_name}: {str(e)}\")\n",
        "\n",
        "crawler_names = [b['name'] for b in s3_buckets]\n",
        "start_all_crawlers(crawler_names)\n",
        "```\n",
        "\n",
        "### **Hard**\n",
        "\n",
        "**Q48. Design a Lambda architecture for real-time and batch processing using AWS, PySpark, and Databricks.**\n",
        "\n",
        "**Answer**:\n",
        "```\n",
        "Architecture Overview:\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│                       Data Sources                           │\n",
        "│  (IoT Devices, APIs, Databases, Log Files)                  │\n",
        "└────────────────────┬────────────────────────────────────────┘\n",
        "                     │\n",
        "         ┌───────────┴───────────┐\n",
        "         │                       │\n",
        "    ┌────▼─────┐          ┌─────▼────┐\n",
        "    │ Kinesis  │          │    S3    │\n",
        "    │ Stream   │          │  Bucket  │\n",
        "    └────┬─────┘          └─────┬────┘\n",
        "         │                      │\n",
        "         │ Speed Layer          │ Batch Layer\n",
        "         │                      │\n",
        "    ┌────▼─────────┐      ┌─────▼──────────┐\n",
        "    │  Lambda +    │      │  EMR/Databricks│\n",
        "    │  Kinesis     │      │  Batch Jobs    │\n",
        "    │  Analytics   │      │                │\n",
        "    └────┬─────────┘      └─────┬──────────┘\n",
        "         │                      │\n",
        "         │                      │\n",
        "    ┌────▼──────────────────────▼─────┐\n",
        "    │      Serving Layer               │\n",
        "    │   (DynamoDB + S3 + Athena)      │\n",
        "    └────┬─────────────────────────────┘\n",
        "         │\n",
        "    ┌────▼─────┐\n",
        "    │   API    │\n",
        "    │ Gateway  │\n",
        "    └──────────┘\n",
        "\n",
        "Implementation:\n",
        "```\n",
        "\n",
        "```python\n",
        "# 1. Speed Layer - Real-time processing with Lambda\n",
        "import json\n",
        "import boto3\n",
        "from datetime import datetime\n",
        "\n",
        "def lambda_realtime_processor(event, context):\n",
        "    \"\"\"\n",
        "    Process streaming data from Kinesis\n",
        "    \"\"\"\n",
        "    dynamodb = boto3.resource('dynamodb')\n",
        "    table = dynamodb.Table('realtime_metrics')\n",
        "    \n",
        "    for record in event['Records']:\n",
        "        # Decode Kinesis data\n",
        "        payload = json.loads(\n",
        "            base64.b64decode(record['kinesis']['data'])\n",
        "        )\n",
        "        \n",
        "        # Process and aggregate\n",
        "        metric = {\n",
        "            'metric_id': payload['sensor_id'],\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'value': payload['temperature'],\n",
        "            'ttl': int(time.time()) + 86400  # 24 hour TTL\n",
        "        }\n",
        "        \n",
        "        # Write to DynamoDB for real-time queries\n",
        "        table.put_item(Item=metric)\n",
        "    \n",
        "    return {'statusCode': 200}\n",
        "\n",
        "# 2. Batch Layer - Historical processing with Databricks\n",
        "from pyspark.sql.functions import col, window, avg, max, min\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "def batch_processor():\n",
        "    \"\"\"\n",
        "    Daily batch processing of historical data\n",
        "    \"\"\"\n",
        "    # Read raw data from S3\n",
        "    df_raw = spark.read \\\n",
        "        .format(\"json\") \\\n",
        "        .load(\"s3://data-lake/raw/sensors/\")\n",
        "    \n",
        "    # Complex aggregations\n",
        "    df_aggregated = df_raw \\\n",
        "        .groupBy(\n",
        "            \"sensor_id\",\n",
        "            window(\"timestamp\", \"1 hour\")\n",
        "        ).agg(\n",
        "            avg(\"temperature\").alias(\"avg_temp\"),\n",
        "            max(\"temperature\").alias(\"max_temp\"),\n",
        "            min(\"temperature\").alias(\"min_temp\"),\n",
        "            count(\"*\").alias(\"reading_count\")\n",
        "        )\n",
        "    \n",
        "    # Write to Delta Lake\n",
        "    df_aggregated.write \\\n",
        "        .format(\"delta\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .partitionBy(\"date\") \\\n",
        "        .save(\"s3://data-lake/processed/sensor_hourly/\")\n",
        "    \n",
        "    # Merge with existing aggregations\n",
        "    delta_table = DeltaTable.forPath(\n",
        "        spark, \"s3://data-lake/serving/sensor_aggregates/\"\n",
        "    )\n",
        "    \n",
        "    delta_table.alias(\"target\").merge(\n",
        "        df_aggregated.alias(\"source\"),\n",
        "        \"\"\"target.sensor_id = source.sensor_id AND\n",
        "           target.hour = source.window.start\"\"\"\n",
        "    ).whenMatchedUpdateAll() \\\n",
        "     .whenNotMatchedInsertAll() \\\n",
        "     .execute()\n",
        "\n",
        "# 3. Serving Layer - Unified query interface\n",
        "def query_unified_view(sensor_id, start_time, end_time):\n",
        "    \"\"\"\n",
        "    Query combining real-time and batch data\n",
        "    \"\"\"\n",
        "    # Query batch data (historical)\n",
        "    batch_query = f\"\"\"\n",
        "        SELECT * FROM sensor_aggregates\n",
        "        WHERE sensor_id = '{sensor_id}'\n",
        "        AND timestamp BETWEEN '{start_time}' AND '{end_time}'\n",
        "    \"\"\"\n",
        "    df_batch = spark.sql(batch_query)\n",
        "    \n",
        "    # Query real-time data from DynamoDB\n",
        "    dynamodb = boto3.resource('dynamodb')\n",
        "    table = dynamodb.Table('realtime_metrics')\n",
        "    \n",
        "    response = table.query(\n",
        "        KeyConditionExpression=Key('metric_id').eq(sensor_id) &\n",
        "        Key('timestamp').between(start_time, end_time)\n",
        "    )\n",
        "    \n",
        "    # Combine results\n",
        "    df_realtime = spark.createDataFrame(response['Items'])\n",
        "    df_unified = df_batch.union(df_realtime)\n",
        "    \n",
        "    return df_unified\n",
        "\n",
        "# 4. Orchestration with Step Functions\n",
        "step_function_definition = {\n",
        "    \"Comment\": \"Lambda Architecture Orchestration\",\n",
        "    \"StartAt\": \"TriggerBatchJob\",\n",
        "    \"States\": {\n",
        "        \"TriggerBatchJob\": {\n",
        "            \"Type\": \"Task\",\n",
        "            \"Resource\": \"arn:aws:states:::databricks:startRun.sync\",\n",
        "            \"Parameters\": {\n",
        "                \"JobId\": \"batch_processor_job_id\"\n",
        "            },\n",
        "            \"Next\": \"UpdateServingLayer\"\n",
        "        },\n",
        "        \"UpdateServingLayer\": {\n",
        "            \"Type\": \"Task\",\n",
        "            \"Resource\": \"arn:aws:lambda:us-east-1:123456789:function:update-serving\",\n",
        "            \"End\": True\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "**Q49. Implement a data quality framework that validates data across Oracle, S3, and Databricks.**\n",
        "\n",
        "**Answer**:\n",
        "```python\n",
        "from pyspark.sql.functions import col, count, when, isnan, isnull\n",
        "from datetime import datetime\n",
        "import boto3\n",
        "\n",
        "class DataQualityFramework:\n",
        "    \"\"\"\n",
        "    Unified data quality validation framework\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, spark_session):\n",
        "        self.spark = spark_session\n",
        "        self.results = []\n",
        "        \n",
        "    def validate_completeness(self, df, column_name, threshold=0.95):\n",
        "        \"\"\"\n",
        "        Check for null/missing values\n",
        "        \"\"\"\n",
        "        total_count = df.count()\n",
        "        non_null_count = df.filter(\n",
        "            col(column_name).isNotNull()\n",
        "        ).count()\n",
        "        \n",
        "        completeness_ratio = non_null_count / total_count\n",
        "        \n",
        "        result = {\n",
        "            'check': 'completeness',\n",
        "            'column': column_name,\n",
        "            'total_records': total_count,\n",
        "            'valid_records': non_null_count,\n",
        "            'ratio': completeness_ratio,\n",
        "            'threshold': threshold,\n",
        "            'status': 'PASS' if completeness_ratio >= threshold else 'FAIL',\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        self.results.append(result)\n",
        "        return result\n",
        "    \n",
        "    def validate_uniqueness(self, df, column_name):\n",
        "        \"\"\"\n",
        "        Check for duplicate values\n",
        "        \"\"\"\n",
        "        total_count = df.count()\n",
        "        distinct_count = df.select(column_name).distinct().count()\n",
        "        \n",
        "        result = {\n",
        "            'check': 'uniqueness',\n",
        "            'column': column_name,\n",
        "            'total_records': total_count,\n",
        "            'distinct_records': distinct_count,\n",
        "            'duplicates': total_count - distinct_count,\n",
        "            'status': 'PASS' if total_count == distinct_count else 'FAIL',\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        self.results.append(result)\n",
        "        return result\n",
        "    \n",
        "    def validate_range(self, df, column_name, min_val, max_val):\n",
        "        \"\"\"\n",
        "        Check if values are within expected range\n",
        "        \"\"\"\n",
        "        invalid_count = df.filter(\n",
        "            (col(column_name) < min_val) |\n",
        "            (col(column_name) > max_val)\n",
        "        ).count()\n",
        "        \n",
        "        total_count = df.count()\n",
        "        \n",
        "        result = {\n",
        "            'check': 'range',\n",
        "            'column': column_name,\n",
        "            'min': min_val,\n",
        "            'max': max_val,\n",
        "            'invalid_count': invalid_count,\n",
        "            'total_count': total_count,\n",
        "            'status': 'PASS' if invalid_count == 0 else 'FAIL',\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        self.results.append(result)\n",
        "        return result\n",
        "    \n",
        "    def validate_referential_integrity(self, df1, df2, key_column):\n",
        "        \"\"\"\n",
        "        Check foreign key relationships\n",
        "        \"\"\"\n",
        "        orphaned = df1.join(\n",
        "            df2,\n",
        "            df1[key_column] == df2[key_column],\n",
        "            \"left_anti\"\n",
        "        )\n",
        "        \n",
        "        orphaned_count = orphaned.count()\n",
        "        \n",
        "        result = {\n",
        "            'check': 'referential_integrity',\n",
        "            'key_column': key_column,\n",
        "            'orphaned_records': orphaned_count,\n",
        "            'status': 'PASS' if orphaned_count == 0 else 'FAIL',\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        self.results.append(result)\n",
        "        return result\n",
        "    \n",
        "    def validate_schema(self, df, expected_schema):\n",
        "        \"\"\"\n",
        "        Validate DataFrame schema\n",
        "        \"\"\"\n",
        "        actual_columns = set(df.columns)\n",
        "        expected_columns = set(expected_schema.keys())\n",
        "        \n",
        "        missing = expected_columns - actual_columns\n",
        "        extra = actual_columns - expected_columns\n",
        "        \n",
        "        result = {\n",
        "            'check': 'schema',\n",
        "            'missing_columns': list(missing),\n",
        "            'extra_columns': list(extra),\n",
        "            'status': 'PASS' if len(missing) == 0 and len(extra) == 0 else 'FAIL',\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        self.results.append(result)\n",
        "        return result\n",
        "    \n",
        "    def run_all_checks(self, df, config):\n",
        "        \"\"\"\n",
        "        Run all configured data quality checks\n",
        "        \"\"\"\n",
        "        for check in config['checks']:\n",
        "            check_type = check['type']\n",
        "            \n",
        "            if check_type == 'completeness':\n",
        "                self.validate_completeness(\n",
        "                    df, check['column'], check.get('threshold', 0.95)\n",
        "                )\n",
        "            elif check_type == 'uniqueness':\n",
        "                self.validate_uniqueness(df, check['column'])\n",
        "            elif check_type == 'range':\n",
        "                self.validate_range(\n",
        "                    df, check['column'], check['min'], check['max']\n",
        "                )\n",
        "        \n",
        "        # Write results to Delta Lake\n",
        "        results_df = self.spark.createDataFrame(self.results)\n",
        "        results_df.write.format(\"delta\") \\\n",
        "            .mode(\"append\") \\\n",
        "            .save(\"/mnt/delta/data_quality_results\")\n",
        "        \n",
        "        return self.results\n",
        "    \n",
        "    def send_alerts(self, failed_checks):\n",
        "        \"\"\"\n",
        "        Send SNS alerts for failed checks\n",
        "        \"\"\"\n",
        "        sns = boto3.client('sns')\n",
        "        \n",
        "        for check in failed_checks:\n",
        "            if check['status'] == 'FAIL':\n",
        "                message = f\"\"\"\n",
        "                Data Quality Check Failed:\n",
        "                Check Type: {check['check']}\n",
        "                Column: {check.get('column', 'N/A')}\n",
        "                Details: {check}\n",
        "                \"\"\"\n",
        "                \n",
        "                sns.publish(\n",
        "                    TopicArn='arn:aws:sns:us-east-1:123456789:dq-alerts',\n",
        "                    Subject='Data Quality Alert',\n",
        "                    Message=message\n",
        "                )\n",
        "\n",
        "# Usage Example\n",
        "dq_framework = DataQualityFramework(spark)\n",
        "\n",
        "# Configuration\n",
        "config = {\n",
        "    'checks': [\n",
        "        {'type': 'completeness', 'column': 'customer_id', 'threshold': 0.99},\n",
        "        {'type': 'uniqueness', 'column': 'transaction_id'},\n",
        "        {'type': 'range', 'column': 'amount', 'min': 0, 'max': 1000000}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Read data from various sources\n",
        "df_oracle = spark.read.jdbc(jdbc_url, \"transactions\", properties=conn_props)\n",
        "df_s3 = spark.read.parquet(\"s3://bucket/transactions/\")\n",
        "df_delta = spark.read.format(\"delta\").load(\"/mnt/delta/transactions\")\n",
        "\n",
        "# Run checks on all sources\n",
        "results_oracle = dq_framework.run_all_checks(df_oracle, config)\n",
        "results_s3 = dq_framework.run_all_checks(df_s3, config)\n",
        "results_delta = dq_framework.run_all_checks(df_delta, config)\n",
        "\n",
        "# Send alerts for failures\n",
        "all_results = results_oracle + results_s3 + results_delta\n",
        "failed = [r for r in all_results if r['status'] == 'FAIL']\n",
        "dq_framework.send_alerts(failed)\n",
        "```\n",
        "\n",
        "**Q50. Design a disaster recovery and business continuity plan for a mission-critical data platform using AWS and Databricks.**\n",
        "\n",
        "**Answer**:\n",
        "```\n",
        "Disaster Recovery Strategy:\n",
        "═══════════════════════════════════════════════════════════\n",
        "\n",
        "1. RTO (Recovery Time Objective): 4 hours\n",
        "2. RPO (Recovery Point Objective): 15 minutes\n",
        "\n",
        "Architecture Components:\n",
        "────────────────────────────────────────────────────────────\n",
        "\n",
        "Primary Region (us-east-1):\n",
        "├── Databricks Workspace (Primary)\n",
        "├── S3 Buckets with Versioning\n",
        "├── RDS Multi-AZ (Metadata)\n",
        "├── Delta Lake Tables\n",
        "└── AWS Glue Data Catalog\n",
        "\n",
        "DR Region (us-west-2):\n",
        "├── Databricks Workspace (Standby)\n",
        "├── S3 Cross-Region Replication\n",
        "├── RDS Read Replica → Promotable\n",
        "├── Delta Lake Replicas\n",
        "└── AWS Glue Data Catalog Replica\n",
        "\n",
        "Implementation:\n",
        "```\n",
        "\n",
        "```python\n",
        "# 1. Automated S3 Cross-Region Replication\n",
        "import boto3\n",
        "\n",
        "def setup_cross_region_replication():\n",
        "    \"\"\"\n",
        "    Configure S3 replication to DR region\n",
        "    \"\"\"\n",
        "    s3_client = boto3.client('s3')\n",
        "    \n",
        "    replication_config = {\n",
        "        'Role': 'arn:aws:iam::123456789:role/S3ReplicationRole',\n",
        "        'Rules': [{\n",
        "            'ID': 'ReplicateAll',\n",
        "            'Priority': 1,\n",
        "            'Filter': {'Prefix': ''},\n",
        "            'Status': 'Enabled',\n",
        "            'Destination': {\n",
        "                'Bucket': 'arn:aws:s3:::dr-bucket-us-west-2',\n",
        "                'ReplicationTime': {\n",
        "                    'Status': 'Enabled',\n",
        "                    'Time': {'Minutes': 15}\n",
        "                },\n",
        "                'Metrics': {\n",
        "                    'Status': 'Enabled',\n",
        "                    'EventThreshold': {'Minutes': 15}\n",
        "                }\n",
        "            },\n",
        "            'DeleteMarkerReplication': {'Status': 'Enabled'}\n",
        "        }]\n",
        "    }\n",
        "    \n",
        "    s3_client.put_bucket_replication(\n",
        "        Bucket='primary-bucket-us-east-1',\n",
        "        ReplicationConfiguration=replication_config\n",
        "    )\n",
        "\n",
        "# 2. Delta Lake Snapshot and Replication\n",
        "def replicate_delta_tables():\n",
        "    \"\"\"\n",
        "    Replicate Delta Lake tables to DR region\n",
        "    \"\"\"\n",
        "    from delta.tables import DeltaTable\n",
        "    \n",
        "    # List of critical tables\n",
        "    critical_tables = [\n",
        "        '/mnt/delta/transactions',\n",
        "        '/mnt/delta/customers',\n",
        "        '/mnt/delta/products'\n",
        "    ]\n",
        "    \n",
        "    for table_path in critical_tables:\n",
        "        # Create deep clone in DR location\n",
        "        spark.sql(f\"\"\"\n",
        "            CREATE OR REPLACE TABLE delta.`/mnt/dr/delta{table_path}`\n",
        "            DEEP CLONE delta.`{table_path}`\n",
        "        \"\"\")\n",
        "        \n",
        "        # Sync to S3 DR bucket\n",
        "        df = spark.read.format(\"delta\").load(table_path)\n",
        "        df.write.format(\"delta\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .save(f\"s3://dr-bucket-us-west-2{table_path}\")\n",
        "\n",
        "# 3. RDS Automated Backups and Read Replica\n",
        "def setup_rds_dr():\n",
        "    \"\"\"\n",
        "    Configure RDS for disaster recovery\n",
        "    \"\"\"\n",
        "    rds_client = boto3.client('rds')\n",
        "    \n",
        "    # Create read replica in DR region\n",
        "    response = rds_client.create_db_instance_read_replica(\n",
        "        DBInstanceIdentifier='metadata-db-replica-dr',\n",
        "        SourceDBInstanceIdentifier='metadata-db-primary',\n",
        "        DBInstanceClass='db.r5.large',\n",
        "        AvailabilityZone='us-west-2a',\n",
        "        MultiAZ=True,\n",
        "        AutoMinorVersionUpgrade=True,\n",
        "        CopyTagsToSnapshot=True\n",
        "    )\n",
        "    \n",
        "    # Enable automated backups\n",
        "    rds_client.modify_db_instance(\n",
        "        DBInstanceIdentifier='metadata-db-primary',\n",
        "        BackupRetentionPeriod=35,\n",
        "        PreferredBackupWindow='03:00-04:00',\n",
        "        CopyTagsToSnapshot=True\n",
        "    )\n",
        "\n",
        "# 4. Databricks Workspace Backup\n",
        "def backup_databricks_workspace():\n",
        "    \"\"\"\n",
        "    Backup Databricks notebooks, jobs, and configurations\n",
        "    \"\"\"\n",
        "    import requests\n",
        "    \n",
        "    databricks_token = \"your_token\"\n",
        "    workspace_url = \"https://your-workspace.cloud.databricks.com\"\n",
        "    \n",
        "    headers = {'Authorization': f'Bearer {databricks_token}'}\n",
        "    \n",
        "    # Export all notebooks\n",
        "    notebooks_response = requests.get(\n",
        "        f\"{workspace_url}/api/2.0/workspace/list\",\n",
        "        headers=headers,\n",
        "        params={'path': '/'}\n",
        "    )\n",
        "    \n",
        "    for notebook in notebooks_response.json().get('objects', []):\n",
        "        # Export each notebook\n",
        "        export_response = requests.get(\n",
        "            f\"{workspace_url}/api/2.0/workspace/export\",\n",
        "            headers=headers,\n",
        "            params={\n",
        "                'path': notebook['path'],\n",
        "                'format': 'SOURCE'\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        # Save to S3 (which replicates to DR)\n",
        "        s3_client = boto3.client('s3')\n",
        "        s3_client.put_object(\n",
        "            Bucket='databricks-backup',\n",
        "            Key=f\"notebooks{notebook['path']}\",\n",
        "            Body=export_response.content\n",
        "        )\n",
        "    \n",
        "    # Backup job configurations\n",
        "    jobs_response = requests.get(\n",
        "        f\"{workspace_url}/api/2.1/jobs/list\",\n",
        "        headers=headers\n",
        "    )\n",
        "    \n",
        "    s3_client.put_object(\n",
        "        Bucket='databricks-backup',\n",
        "        Key='jobs/configurations.json',\n",
        "        Body=json.dumps(jobs_response.json())\n",
        "    )\n",
        "\n",
        "# 5. Failover Orchestration with Lambda\n",
        "def failover_orchestrator(event, context):\n",
        "    \"\"\"\n",
        "    Automated failover orchestration\n",
        "    \"\"\"\n",
        "    import boto3\n",
        "    from datetime import datetime\n",
        "    \n",
        "    # 1. Promote RDS read replica\n",
        "    rds = boto3.client('rds', region_name='us-west-2')\n",
        "    rds.promote_read_replica(\n",
        "        DBInstanceIdentifier='metadata-db-replica-dr'\n",
        "    )\n",
        "    \n",
        "    # 2. Update Route53 to point to DR region\n",
        "    route53 = boto3.client('route53')\n",
        "    route53.change_resource_record_sets(\n",
        "        HostedZoneId='Z1234567890ABC',\n",
        "        ChangeBatch={\n",
        "            'Changes': [{\n",
        "                'Action': 'UPSERT',\n",
        "                'ResourceRecordSet': {\n",
        "                    'Name': 'api.example.com',\n",
        "                    'Type': 'A',\n",
        "                    'AliasTarget': {\n",
        "                        'HostedZoneId': 'Z0987654321XYZ',\n",
        "                        'DNSName': 'dr-alb.us-west-2.elb.amazonaws.com',\n",
        "                        'EvaluateTargetHealth': True\n",
        "                    }\n",
        "                }\n",
        "            }]\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # 3. Start DR Databricks cluster\n",
        "    databricks_api = \"https://dr-workspace.cloud.databricks.com\"\n",
        "    requests.post(\n",
        "        f\"{databricks_api}/api/2.0/clusters/start\",\n",
        "        headers={'Authorization': f'Bearer {dr_token}'},\n",
        "        json={'cluster_id': 'dr-cluster-id'}\n",
        "    )\n",
        "    \n",
        "    # 4. Send notifications\n",
        "    sns = boto3.client('sns', region_name='us-west-2')\n",
        "    sns.publish(\n",
        "        TopicArn='arn:aws:sns:us-west-2:123456789:dr-alerts',\n",
        "        Subject='DISASTER RECOVERY ACTIVATED',\n",
        "        Message=f\"\"\"\n",
        "        Disaster Recovery Failover Initiated\n",
        "        Time: {datetime.now().isoformat()}\n",
        "        Primary Region: us-east-1 (DOWN)\n",
        "        DR Region: us-west-2 (ACTIVE)\n",
        "        \n",
        "        Actions Taken:\n",
        "        1. RDS Read Replica Promoted\n",
        "        2. Route53 Updated\n",
        "        3. DR Databricks Cluster Started\n",
        "        4. Applications redirected to DR\n",
        "        \n",
        "        Estimated Recovery Time: 30 minutes\n",
        "        \"\"\"\n",
        "    )\n",
        "    \n",
        "    return {'statusCode': 200, 'body': 'Failover initiated'}\n",
        "\n",
        "# 6. Continuous DR Testing\n",
        "def dr_test_schedule():\n",
        "    \"\"\"\n",
        "    Quarterly DR test procedure\n",
        "    \"\"\"\n",
        "    test_plan = \"\"\"\n",
        "    DR Test Checklist:\n",
        "    ══════════════════════════════════════════════════════\n",
        "    \n",
        "    Pre-Test (1 week before):\n",
        "    □ Notify stakeholders\n",
        "    □ Verify backup integrity\n",
        "    □ Check replication lag (should be < 15 min)\n",
        "    □ Review runbooks\n",
        "    \n",
        "    During Test:\n",
        "    □ Simulate primary region failure\n",
        "    □ Execute failover procedures\n",
        "    □ Validate DR application functionality\n",
        "    □ Test data integrity\n",
        "    □ Measure actual RTO/RPO\n",
        "    □ Document issues\n",
        "    \n",
        "    Post-Test:\n",
        "    □ Failback to primary\n",
        "    □ Update documentation\n",
        "    □ Address identified gaps\n",
        "    □ Report to stakeholders\n",
        "    \n",
        "    Success Criteria:\n",
        "    ✓ RTO < 4 hours\n",
        "    ✓ RPO < 15 minutes\n",
        "    ✓ 100% critical functionality restored\n",
        "    ✓ Zero data loss for committed transactions\n",
        "    \"\"\"\n",
        "    \n",
        "    return test_plan\n",
        "\n",
        "# 7. Monitoring and Alerting\n",
        "def setup_dr_monitoring():\n",
        "    \"\"\"\n",
        "    CloudWatch alarms for DR readiness\n",
        "    \"\"\"\n",
        "    cloudwatch = boto3.client('cloudwatch')\n",
        "    \n",
        "    # Replication lag alarm\n",
        "    cloudwatch.put_metric_alarm(\n",
        "        AlarmName='S3-Replication-Lag',\n",
        "        ComparisonOperator='GreaterThanThreshold',\n",
        "        EvaluationPeriods=2,\n",
        "        MetricName='ReplicationLatency',\n",
        "        Namespace='AWS/S3',\n",
        "        Period=300,\n",
        "        Statistic='Average',\n",
        "        Threshold=900.0,  # 15 minutes\n",
        "        ActionsEnabled=True,\n",
        "        AlarmActions=['arn:aws:sns:us-east-1:123456789:dr-alerts']\n",
        "    )\n",
        "    \n",
        "    # RDS replica lag alarm\n",
        "    cloudwatch.put_metric_alarm(\n",
        "        AlarmName='RDS-Replica-Lag',\n",
        "        ComparisonOperator='GreaterThanThreshold',\n",
        "        EvaluationPeriods=2,\n",
        "        MetricName='ReplicaLag',\n",
        "        Namespace='AWS/RDS',\n",
        "        Period=60,\n",
        "        Statistic='Average',\n",
        "        Threshold=30.0,  # 30 seconds\n",
        "        ActionsEnabled=True,\n",
        "        AlarmActions=['arn:aws:sns:us-east-1:123456789:dr-alerts']\n",
        "    )\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "This comprehensive questionnaire covers all requested technologies with a good mix of theoretical knowledge and practical scenarios. The questions progress from easy to hard, testing both conceptual understanding and hands-on skills expected from candidates with 4-7 years of experience."
      ],
      "metadata": {
        "id": "WC94idu-xYqo"
      }
    }
  ]
}